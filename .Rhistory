summarise(pol.after = mean(polityafter),
pol.before = mean(politybefore),
war.after = mean(warafter),
war.before = mean(warbefore)) %>%
mutate(
d.pol.after = pol.after - pol.before,
d.war.after = war.after - war.before
)
df$d.pol.after[2] - df$d.pol.after[1]
df$d.war.after[2] - df$d.war.after[1]
library("ISLR")
df = Carseats
model.1 = lm(Sales ~  Price + Urban + US,
data = df)
model.1
View(df)
View(df)
install.packages("broom")
library("broom")
output.1 = model.1 %>% tidy
install.packages("broom")
View(output.1)
View(output.1)
model.1
df = df %>%
mutate (
Urban = if_else("Yes" == 1, 0)
US = if_else("Yes" ==1, 0)
)
df = df %>%
mutate (
Urban1 = if_else(Urban =="Yes", 1, 0)
US1 = if_else(US=="Yes" ,1, 0)
)
df = df %>%
mutate (
Urban1 = ifelse(Urban =="Yes", 1, 0)
US1 = ifelse(US=="Yes" ,1, 0)
)
df = df %>%
mutate (
Urban1 = ifelse(Urban =="Yes", 1, 0),
US1 = ifelse(US=="Yes" ,1, 0)
)
library("dplyr")
df = df %>%
mutate (
Urban1 = ifelse(Urban =="Yes", 1, 0),
US1 = ifelse(US=="Yes" ,1, 0)
)
View(df)
View(df)
model.2 = lm(Sales ~  Price + Urban1 + US1,
data = df)
model.2
output.1 = model.2 %>% tidy
library("tidyr")
model.2
output.1 = model.2 %>% tidy
p = ggplot(output.1, aes(x = term, y = estimate))
p = p + geom_hline(aes(yintercept = 0),  size = 2,
colour = "white") +
geom_point() +
geom_errorbar(aes(ymin=estimate-2*std.error,
ymax=estimate+2*std.error)) +
coord_flip()
p = ggplot(output.1, aes(x = term, y = estimate))
library("ggplot2")
p = ggplot(output.1, aes(x = term, y = estimate))
p = p + geom_hline(aes(yintercept = 0),  size = 2,
colour = "white") +
geom_point() +
geom_errorbar(aes(ymin=estimate-2*std.error,
ymax=estimate+2*std.error)) +
coord_flip()
p
model.2
output.1 = model.2 %>% tidy
library("tidyr")
output.1 = model.1 %>% tidy
output.1 = model.2 %>% tidy
install.packages("broom")
library("broom")
output.1 = model.2 %>% tidy
p = ggplot(output.1, aes(x = term, y = estimate))
p = p + geom_hline(aes(yintercept = 0),  size = 2,
colour = "white") +
geom_point() +
geom_errorbar(aes(ymin=estimate-2*std.error,
ymax=estimate+2*std.error)) +
coord_flip()
p
model.3 = lm(Sales ~  Price  + US1,
data = df)
model.3
output.2 = model.3 %>% tidy
View(output.2)
View(output.2)
summary(model.3)
p = ggplot(output.2, aes(x = term, y = estimate))
p = p + geom_hline(aes(yintercept = 0),  size = 2,
colour = "white") +
geom_point() +
geom_errorbar(aes(ymin=estimate-2*std.error,
ymax=estimate+2*std.error)) +
coord_flip()
p
plot(Sales ~  Price  + US1,
data=df)
dfauto = Auto
library("ISLR")
install.packages("ISLR")
install.packages("ISLR")
library("ISLR")
dfauto = Auto
View(dfauto)
View(dfauto)
p = ggplot(output.2, aes(x = term, y = estimate))
p = p + geom_hline(aes(yintercept = 0),  size = 2,
colour = "white") +
geom_point() +
geom_errorbar(aes(ymin=estimate-2*std.error,
ymax=estimate+2*std.error)) +
coord_flip()
p
library("readr")
library("dplyr")
infile = "../nopub/yelp_academic_dataset_review.json"
review_lines = read_lines(infile,
n_max = 50000,
progress = FALSE)
library("stringr")
library("jsonlite")
reviews_combined = str_c("[",
str_c(review_lines,
collapse = ", "),
"]")
reviews_combined = str_c("[",
str_c(review_lines,
collapse = ", "),
"]")
library("readr")
library("dplyr")
infile = "../nopub/yelp_academic_dataset_review.json"
review_lines = read_lines(infile,
n_max = 50000,
progress = FALSE)
library("readr")
library("dplyr")
infile = "../nopub/yelp_academic_dataset_review.json"
review_lines = read_lines(infile,
n_max = 50000,
progress = FALSE)
library("readr")
library("dplyr")
infile = "../nopub/yelp_academic_dataset_review.json"
review_lines = read_lines(infile,
n_max = 50000,
progress = FALSE)
infile = "/Users/sabineverning13/desktop/nopub/yelp_academic_dataset_review.json"
review_lines = read_lines(infile,
n_max = 50000,
progress = FALSE)
infile = "/Users/sabineverning13/desktop/yelp_dataset_challenge_academic_dataset/yelp_academic_dataset_review.json"
review_lines = read_lines(infile,
n_max = 50000,
progress = FALSE)
library("readr")
library("dplyr")
library("stringr")
library("jsonlite")
reviews_combined = str_c("[",
str_c(review_lines,
collapse = ", "),
"]")
reviews = fromJSON(reviews_combined) %>%
flatten() %>%
tbl_df()
View(reviews)
View(reviews)
library("tidytext")
review_words = reviews %>%
select(review_id, business_id, stars, text) %>%
unnest_tokens(word, text)
review_words %>% dim
install.packages("tidytext")
library("tidytext")
review_words = reviews %>%
select(review_id, business_id, stars, text) %>%
unnest_tokens(word, text)
review_words %>% dim
review_words = review_words %>%
filter(!word %in% stop_words$word) %>%
filter(str_detect(word, "^[a-z']+$"))
View(review_words)
View(review_words)
View(reviews)
View(reviews)
View(review_words)
View(review_words)
AFINN = sentiments %>%
filter(lexicon == "AFINN") %>%
select(word, afinn_score = score)
View(AFINN)
View(AFINN)
reviews_sentiment = review_words %>%
inner_join(AFINN, by = "word") %>%
group_by(review_id, stars) %>%
summarize(sentiment = mean(afinn_score))
View(reviews_sentiment)
View(reviews_sentiment)
df.review = reviews_sentiment %>%
group_by(stars) %>%
summarise(m.sentiment = mean(sentiment))
View(df.review)
View(df.review)
library("ggplot2")
library("viridis")
library("ggrepel")
library("ggalt")
install.packages("ggrepel")
install.packages("ggalt")
library("ggplot2")
library("viridis")
library("ggrepel")
library("ggalt")
p = ggplot(df.review, aes(x = m.sentiment, y = stars,
color = as.factor(stars)))
p + geom_vline(xintercept = 0, size = 2, color = "white") +
geom_lollipop(point.size=3,
horizontal=TRUE) +
geom_text_repel(aes(label = stars),
color = "black",
nudge_y = .2) +
scale_color_viridis(discrete = TRUE) +
labs(x = "Sentiment Score (mean)",
y = "Number of Stars on Yelp") +
theme(legend.position = "none")
p = ggplot(df.review, aes(x = m.sentiment, y = stars,
color = as.factor(stars)))
p + geom_vline(xintercept = 0, size = 2, color = "white") +
geom_lollipop(point.size=3,
horizontal=TRUE) +
geom_text_repel(aes(label = stars),
color = "black",
nudge_y = .2) +
scale_color_viridis(discrete = TRUE) +
labs(x = "Sentiment Score (mean)",
y = "Number of Stars on Yelp") +
theme(legend.position = "none")
p = ggplot(df.review, aes(x = m.sentiment, y = stars,
color = as.factor(stars)))
p + geom_vline(xintercept = 0, size = 2, color = "white") +
geom_text_repel(aes(label = stars),
color = "black",
nudge_y = .2) +
scale_color_viridis(discrete = TRUE) +
labs(x = "Sentiment Score (mean)",
y = "Number of Stars on Yelp") +
theme(legend.position = "none")
review_words_counted = review_words %>%
count(review_id, business_id, stars, word) %>%
ungroup()
View(review_words_counted)
View(review_words_counted)
word_summaries = review_words_counted %>%
group_by(word) %>%
summarize(businesses = n_distinct(business_id),
reviews = n(),
uses = sum(n),
average_stars = mean(stars)) %>%
ungroup() %>%
arrange(reviews)
p = ggplot(word_summaries,
aes(x = reviews))
p + scale_x_log10() +
geom_freqpoly()
#
word_summaries_filtered = word_summaries %>%
filter(reviews >= 200, businesses >= 10)
View(word_summaries)
View(word_summaries_filtered)
p = ggplot(word_summaries_filtered,
aes(x = reviews))
p + scale_x_log10() +
geom_freqpoly()
df.0 = word_summaries_filtered %>%
arrange(-average_stars)
View(df.0)
df.1 = word_summaries_filtered %>%
arrange(average_stars)
p = ggplot(sample_frac(word_summaries_filtered, 1),
aes(reviews, average_stars)) +
geom_point(alpha = .35) +
geom_text(
data = word_summaries_filtered %>%
arrange(-average_stars) %>%
filter(row_number() <= 30),
aes(label = word),
check_overlap = TRUE, vjust = 1, hjust = 0.6) +
geom_text(
data = word_summaries_filtered %>%
arrange(average_stars) %>%
filter(row_number() <= 30),
aes(label = word),
check_overlap = TRUE, vjust = 1, hjust = 0.6) +
geom_text(
data = word_summaries_filtered %>%
arrange(-reviews) %>%
filter(row_number() <= 30),
aes(label = word),
check_overlap = TRUE, vjust = 1, hjust = 1) +
scale_x_log10() +
geom_smooth(se = FALSE) +
xlab("# of reviews") +
ylab("Average Stars")
ggsave(plot = p, file = "figures/reviews.pdf",
width = 6, height = 4)
p = ggplot(sample_frac(word_summaries_filtered, 1),
aes(reviews, average_stars)) +
geom_point(alpha = .35) +
geom_text(
data = word_summaries_filtered %>%
arrange(-average_stars) %>%
filter(row_number() <= 30),
aes(label = word),
check_overlap = TRUE, vjust = 1, hjust = 0.6) +
geom_text(
data = word_summaries_filtered %>%
arrange(average_stars) %>%
filter(row_number() <= 30),
aes(label = word),
check_overlap = TRUE, vjust = 1, hjust = 0.6) +
geom_text(
data = word_summaries_filtered %>%
arrange(-reviews) %>%
filter(row_number() <= 30),
aes(label = word),
check_overlap = TRUE, vjust = 1, hjust = 1) +
scale_x_log10() +
geom_smooth(se = FALSE) +
xlab("# of reviews") +
ylab("Average Stars")
p = ggplot(reviews_sentiment,
aes(x = sentiment, y = stars))
p + geom_jitter() +
geom_smooth()
install.packages("ggsave")
file = paste0("http://varianceexplained.org/",
"files/",
"trump_tweets_df.rda")
load(url(file))
library("tidyr")
tweets = trump_tweets_df %>%
select(id, statusSource, text, created) %>%
extract(statusSource,
"source", "Twitter for (.*?)<") %>%
filter(source %in% c("iPhone", "Android"))
tweets = trump_tweets_df %>%
select(id, statusSource, text, created) %>%
extract(statusSource,
"source", "Twitter for (.*?)<") %>%
filter(source %in% c("iPhone", "Android"))
library("list")
library("tidyr")
install.packages("list")
library("list")
tweets = trump_tweets_df %>%
select(id, statusSource, text, created) %>%
extract(statusSource,
"source", "Twitter for (.*?)<") %>%
filter(source %in% c("iPhone", "Android"))
#
tweets = trump_tweets_df %>%
select(id, statusSource, text, created) %>%
extract(statusSource,
"source", "Twitter for (.*?)<") %>%
filter(source %in% c("iPhone", "Android"))
file = paste0("http://varianceexplained.org/",
"files/",
"trump_tweets_df.rda")
load(url(file))
library("tidyr")
tweets = trump_tweets_df %>%
select(id, statusSource, text, created) %>%
extract(statusSource,
"source", "Twitter for (.*?)<") %>%
filter(source %in% c("iPhone", "Android"))
library(tidyr)
library("tidytext")
library("stringr")
reg = "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
tweet_words = tweets %>%
filter(!str_detect(text, '^"')) %>%
mutate(text =
str_replace_all(text,
"https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
unnest_tokens(word, text, token = "regex",
pattern = reg) %>%
filter(!word %in% stop_words$word,
str_detect(word, "[a-z]"))
file = paste0("http://varianceexplained.org/",
"files/",
"trump_tweets_df.rda")
load(url(file))
View(trump_tweets_df)
library("tidyr")
tweets = trump_tweets_df %>%
select(id, statusSource, text, created) %>%
extract(statusSource,
"source", "Twitter for (.*?)<") %>%
filter(source %in% c("iPhone", "Android"))
library("ggalt", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
library("ggrepel", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
filter(source %in% c("iPhone", "Android"))
"source", "Twitter for (.*?)<") %>%
filter(source %in% c("iPhone", "Android"))
tweets = trump_tweets_df %>%
select(id, statusSource, text, created) %>%
extract(statusSource,
"source", "Twitter for (.*?)<") %>%
filter(source %in% c("iPhone", "Android"))
library("dplyr")
tweets = trump_tweets_df %>%
select(id, statusSource, text, created) %>%
extract(statusSource,
"source", "Twitter for (.*?)<") %>%
filter(source %in% c("iPhone", "Android"))
library("tidytext")
library("stringr")
reg = "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
tweet_words = tweets %>%
filter(!str_detect(text, '^"')) %>%
mutate(text =
str_replace_all(text,
"https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
unnest_tokens(word, text, token = "regex",
pattern = reg) %>%
filter(!word %in% stop_words$word,
str_detect(word, "[a-z]"))
View(tweet_words)
df.tweet = tweet_words %>%
count(word) %>%
arrange(-n) %>%
filter(row_number() <= 10) %>%
ungroup() %>%
mutate(
word = reorder(as.factor(word), n)
)
p = ggplot(df.tweet, aes(x = word,
y = n))
p + geom_bar(stat = "identity") +
coord_flip() +
labs(x = NULL)
android_iphone_ratios = tweet_words %>%
count(word, source) %>%
filter(sum(n) >= 5) %>%
spread(source, n, fill = 0) %>%
ungroup() %>%
mutate_each(funs((. + 1) / sum(. + 1)), -word) %>%
mutate(logratio = log2(Android / iPhone))
View(android_iphone_ratios)
#
df.1 = android_iphone_ratios %>%
arrange(-logratio) %>%
filter(row_number() <= 10) %>%
mutate(device = "Android")
df.2 = android_iphone_ratios %>%
arrange(logratio) %>%
filter(row_number() <= 10) %>%
mutate(device = "iPhone")
df.trump = df.1 %>%
bind_rows(df.2) %>%
mutate(
word = reorder(word, logratio)
)
View(df.trump)
p = ggplot(df.trump,
aes(x = word, y = logratio,
fill = device))
p + geom_bar(stat = "identity", color = "black") +
coord_flip() + scale_fill_viridis(discrete = TRUE) +
theme(legend.position = "below") +
labs(x = NULL, y = NULL)
library("viridis")
p = ggplot(df.trump,
aes(x = word, y = logratio,
fill = device))
p + geom_bar(stat = "identity", color = "black") +
coord_flip() + scale_fill_viridis(discrete = TRUE) +
theme(legend.position = "below") +
labs(x = NULL, y = NULL)
nrc = sentiments %>%
filter(lexicon == "nrc") %>%
select(word, sentiment)
View(nrc)
p = android_iphone_ratios %>%
inner_join(nrc, by = "word") %>%
filter(!sentiment %in% c("positive", "negative")) %>%
mutate(sentiment = reorder(sentiment, -logratio),
word = reorder(word, -logratio)) %>%
group_by(sentiment) %>%
top_n(10, abs(logratio)) %>%
ungroup() %>%
ggplot(aes(word, logratio, fill = logratio < 0)) +
facet_wrap(~ sentiment, scales = "free", nrow = 2) +
geom_bar(stat = "identity", color = "black") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
labs(x = "", y = "Android / iPhone log ratio") +
scale_fill_viridis(name = "", labels = c("Android", "iPhone"),
discrete = TRUE)
p
